import os  # Permite acessar arquivos, pastas e configura√ß√µes do computador
import streamlit as st  # Framework para criar aplica√ß√µes web interativas
from langchain_community.document_loaders import PyPDFLoader  # Carrega arquivos PDF
from langchain.text_splitter import RecursiveCharacterTextSplitter  # Divide textos grandes em peda√ßos menores
from langchain_openai import OpenAIEmbeddings  # Converte texto em n√∫meros (vetores)
from langchain_community.vectorstores import FAISS  # Banco de dados para armazenar e buscar vetores
from langchain_openai import ChatOpenAI  # Modelo de linguagem da OpenAI (GPT)
from langchain.chains import RetrievalQA  # Sistema de perguntas e respostas
from langchain.prompts import ChatPromptTemplate  # Template para formatar perguntas ao modelo
from dotenv import load_dotenv  # Carrega senhas do arquivo .env (arquivo secreto)
import tiktoken  # Conta quantas "palavras" (tokens) tem um texto
from langchain_core.runnables import RunnablePassthrough, RunnableParallel # processamento paralelo na chain
from langchain_core.output_parsers import StrOutputParser  #organizador de output
from langchain.schema.runnable import RunnableLambda  #encapsulador de fun√ß√£o
from langchain.schema import Document
from langchain_community.document_loaders import DirectoryLoader


# ============================ INICIA O STREAMLIT =====================================
# Este c√≥digo precisa acontecer antes de qualquer chamada de outra fun√ß√£o do streamlit
# Define t√≠tulo da aba, √≠cone e layout geral da p√°gina
st.set_page_config(
    page_title="BIA",  # T√≠tulo que aparece na aba do navegador
    page_icon="üêü",  # √çcone que aparece na aba
    layout="wide"  # Usa toda a largura da tela
)
# =====================================================================================


# ============================ CHAVES DO SISTEMA ===============================
# chave para deploy
deploy = True #False == vers√£o local / True == vers√£o deploy
#chve para cria√ß√£o de vector store
cria_vector = False #False == s√≥ carrega a vector store / True == cria a vector store
# ===============================================================================

# Tenta pegar a chave da API da OpenAI de duas formas diferentes
load_dotenv()
if not deploy:
    api_key = os.getenv("OPENAI_API_KEY")  # Do arquivo .env
else:
    api_key = st.secrets["OPENAI_API_KEY"]  # Do Streamlit secrets (para deploy)

# Verifica se conseguiu pegar a chave da API
if not api_key:
    # Se n√£o conseguiu, para tudo e mostra erro
    raise ValueError("A vari√°vel de ambiente 'OPENAI_API_KEY' n√£o foi encontrada no seu arquivo .env.")


# -----------------------------------------------------------------------------
# 1. DEFINI√á√ïES GERAIS
# -----------------------------------------------------------------------------
#"""  Defini√ß√µes gerais (vari√°veis de sistema, modelo, vectorstore, arquivos, ...) """

# Define a chave API como vari√°vel de ambiente
os.environ["OPENAI_API_KEY"] = api_key

#Define o modelo de GPT
modelo = 'gpt-3.5-turbo-0125'  # Qual modelo do ChatGPT usar

# Cria uma inst√¢ncia do modelo de chat da OpenAI
chat_instance = ChatOpenAI(model=modelo)

#Define o modelo de embedding
embeddings_model = OpenAIEmbeddings()  # Modelo para converter texto em n√∫meros

#Define o local onde o vector store persistir√°
diretorio_vectorstore_faiss = 'vectorstore_faiss'  # Onde salvar o banco vetorial

# #Define o local do arquivo a ser processado
# caminho_arquivo = r'BIA_RAG.pdf'  # Caminho do PDF para analisar

#Define o diret√≥rio onde os arquivos para gerar o vector store est√£o localizados
caminho_arquivo = 'docs'  # Caminho dos arquivos para analisar

#Define a quantidade m√°xima de documentos retornados pela fun√ß√£o retriever()
qtd_retriever = 4


# -----------------------------------------------------------------------------
# 2. PROMPTS
# -----------------------------------------------------------------------------
#"""  Prompts """

# Template para pergunta original
prompt_inicial = ChatPromptTemplate.from_template("""
    Voc√™ √© um copywiter e deve ajudar a esclarecer d√∫vidas sobre o projeto Biometria Inteligente para Aquicultura (BIA)
    Use o contexto fornecido para responder √†s perguntas. Voc√™ pode buscar dados complementares na internet, desde que
    forne√ßa as fontes consultadas. Se voc√™ n√£o souber a resposta, diga que n√£o sabe, n√£o tente inventar.
    
    Contexto: {context}
    
    Pergunta: {question}
    
    Resposta detalhada:
    """)

# Template para traduzir para o ingL√™s a reposta
translation_prompt = ChatPromptTemplate.from_template("""
    Voc√™ √© um tradutor especializado em textos t√©cnicos.
    Traduza o seguinte texto para ingl√™s, mantendo termos t√©cnicos e formata√ß√£o:
    
    Texto: {text}
    
    Tradu√ß√£o:
    """)


# -----------------------------------------------------------------------------
# 3. FUN√á√ïES
# -----------------------------------------------------------------------------
#"""  Fun√ß√µes """

def cria_vector_store_faiss(chunk: list[Document], diretorio_vectorestore_faiss:str) -> FAISS:
    """
    Esta fun√ß√£o cria um banco de dados especial para busca r√°pida de informa√ß√µes

    O que ela faz:
    1. Pega os peda√ßos de texto do documento
    2. Converte cada peda√ßo em n√∫meros (vetores) que representam o significado
    3. Cria um banco de dados FAISS para busca super r√°pida
    4. Salva tudo no computador para usar depois

    Par√¢metros:
        chunks: Lista de peda√ßos de documento j√° divididos

    Retorna:
        vectorstore: O banco de dados criado
    """
    # Cria o banco vetorial usando os chunks e o modelo de embeddings
    # FAISS √© uma biblioteca que faz busca muito r√°pida em vetores
    vector_store = FAISS.from_documents(chunk, embeddings_model)

    # Salva o banco no disco r√≠gido para n√£o precisar recriar toda vez
    vector_store.save_local(diretorio_vectorestore_faiss)

    return vector_store


def carrega_vector_store_faiss(diretorio_vectorestore_faiss, embedding_model):
    """
    Esta fun√ß√£o carrega um banco de dados vetorial j√° criado anteriormente

    O que ela faz:
    1. Procura o banco de dados salvo no disco
    2. Carrega ele na mem√≥ria
    3. Retorna pronto para uso

    Par√¢metros:
        diretorio_vectorestore_faiss: Pasta onde est√° salvo o banco
        embeddings_model: Modelo usado para criar os embeddings

    Retorna:
        vectorstore: O banco de dados carregado
    """
    # Carrega o banco vetorial do disco r√≠gido
    vector_store = FAISS.load_local(
        diretorio_vectorestore_faiss,
        embedding_model,
        allow_dangerous_deserialization=True
    )

    return vector_store


def num_tokens_from_string(string: str, encoding_name: str = "cl100k_base") -> int:
    """
    Esta fun√ß√£o conta quantos "tokens" (pedacinhos) tem em um texto

    Por que isso √© importante?
    - A IA tem limite de quantos tokens pode processar de uma vez
    - Precisamos saber se o texto cabe no limite
    - 1 token ‚âà 0.75 palavras em portugu√™s

    Par√¢metros:
        string: O texto para contar
        encoding_name: Tipo de codifica√ß√£o (padr√£o da OpenAI)

    Retorna:
        int: N√∫mero de tokens no texto
    """
    # Pega o codificador espec√≠fico da OpenAI
    encoding = tiktoken.get_encoding(encoding_name)

    # Codifica o texto em tokens e conta quantos s√£o
    num_tokens = len(encoding.encode(string))

    return num_tokens


def cria_chunks(caminho: str, chunk_size: int, chunk_overlap: int) -> list:
    print(f"Carregando documentos do PDF: {caminho_arquivo}")  # Informa o usu√°rio sobre o carregamento do PDF.
    # try:  # Inicia um bloco try-except para lidar com poss√≠veis erros durante o carregamento do arquivo.
    #     loader = PyPDFLoader(arquivo)  # Instancia um PyPDFLoader, passando o caminho do arquivo PDF.
    #     documentos = loader.load()  # Carrega o conte√∫do do PDF. Cada p√°gina do PDF se torna um 'Document' separado na lista 'documentos'.
    #     print(
    #         f"PDF carregado. Total de documentos (p√°ginas): {len(documentos)}")  # Confirma o carregamento e exibe o n√∫mero de p√°ginas/documentos.
    # except FileNotFoundError:  # Captura o erro espec√≠fico se o arquivo PDF n√£o for encontrado.
    #     print(
    #         f"Erro: O arquivo PDF n√£o foi encontrado em '{arquivo}'. Por favor, verifique o caminho.")  # Informa o erro ao usu√°rio.
    #     exit()  # Interrompe a execu√ß√£o do script se o arquivo n√£o for encontrado, pois √© uma depend√™ncia cr√≠tica.

    # Instancia√ß√£o do DirectoryLoader
    loader = DirectoryLoader(
        path= caminho,  # OBRIGAT√ìRIO: Caminho do diret√≥rio
        glob="*.pdf",  # OPCIONAL: Padr√£o para filtrar arquivos (padr√£o: *)
        loader_cls = PyPDFLoader,  # OPCIONAL: Loader espec√≠fico para os arquivos encontrados
        #           Se n√£o especificado, ele tenta inferir,
        #           mas √© mais seguro especificar.
        recursive=False  # OPCIONAL: Buscar em subdiret√≥rios (padr√£o: False)
    )

    # Executa o carregamento
    documentos = loader.load()


    # --- Depura√ß√£o e Inspe√ß√£o de Documentos (Opcional) ---
    # print("\n--- Conte√∫do da primeira p√°gina para verifica√ß√£o ---")
    # print(
    #     documentos[0].page_content[:500] + "...")  # Imprime os primeiros 500 caracteres da primeira p√°gina para depura√ß√£o.
    # print("\n--- Metadados da primeira p√°gina ---")
    # print(documentos[0].metadata)  # Imprime os metadados associados √† primeira p√°gina.
    # print("-" * 50 + "\n")

    # Configura√ß√£o do Text Splitter
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,  # Tamanho m√°ximo do chunk em tokens (usando a fun√ß√£o de contagem do m√≥dulo 'tokens').
        chunk_overlap=chunk_overlap,  # Sobreposi√ß√£o em tokens entre chunks.
        length_function= num_tokens_from_string,  # Nossa fun√ß√£o personalizada de contagem de tokens.
        separators= ['&', '\n\n','.', ' '],
        add_start_index=True  # Adiciona o √≠ndice de in√≠cio de cada chunk no texto original como metadado.
    )

    # Divide o documento completo em chunks
    chunk = text_splitter.split_documents(documentos)
    print(f"Texto original dividido em {len(chunk)} chunks.\n")

    return chunk


def format_docs(docs: list[Document]):
    """
    Pega uma lista de documentos e junta o conte√∫do em um texto s√≥.
    Cada documento √© separado por duas quebras de linha.
    """
    return "\n\n".join(doc.page_content for doc in docs)


def retriever(pergunta: str, n: int):
    """
    O que ela faz:
    1. Recebe sua pergunta
    2. Busca "n" informa√ß√µes relevantes no documento
    3. Retorna lista da classe "Document"

    Par√¢metros:
        pergunta: procura a ser feita na vector store
        n: quantidade m√°xima de documentos a serem retornados na lista

    Retorna:
        documentos_retornados : lista com documentos encontrados da classe Document
    """

    resultado = vectorstore.as_retriever(search_type='mmr',search_kwargs={"k": n})  # Como buscar no documento

    documentos_retornados = resultado.get_relevant_documents(pergunta) #passa a pergunta para o retriever

    return documentos_retornados



# -----------------------------------------------------------------------------
# 4. VECTOR STORE
# -----------------------------------------------------------------------------

if cria_vector: #Chave do sistema
    chunks = cria_chunks(caminho_arquivo,500,50)
    vectorstore = cria_vector_store_faiss(chunks,diretorio_vectorstore_faiss)
else:
    vectorstore = carrega_vector_store_faiss(diretorio_vectorstore_faiss,embeddings_model)


# -----------------------------------------------------------------------------
# 5. CONFIGURA√á√ÉO DA P√ÅGINA PELO STREAMLIT
# -----------------------------------------------------------------------------
# # Define t√≠tulo da aba, √≠cone e layout geral da p√°gina
# st.set_page_config(
#     page_title="BIA",  # T√≠tulo que aparece na aba do navegador
#     page_icon="üêü",  # √çcone que aparece na aba
#     layout="wide"  # Usa toda a largura da tela
# )


# -----------------------------------------------------------------------------
# 6. T√çTULO E DESCRI√á√ÉO DA P√ÅGINA PELO STREAMLIT
# -----------------------------------------------------------------------------
# Mostra o t√≠tulo principal e explica o que a aplica√ß√£o faz
st.title("üêü Biometria Inteligente para Aquicultura")
st.markdown("""
Esta aplica√ß√£o permite que voc√™ analise o projeto BIA usando Intelig√™ncia Artificial.
Fa√ßa perguntas sobre sua aplica√ß√£o e indicadores de investimento e retorno financeiro!
""")

# =============================== PIPELINE =================================
# Cria a sequ√™ncia de chains
# ==========================================================================

# PASSO 1: Chain que faz o RAG (busca + resposta)

# Lambda com a fun√ß√£o personalizada encapsulada em busca_vectorstore
busca_vectorstore = RunnableLambda(
    lambda pergunta: retriever(pergunta, qtd_retriever)
)

rag_chain = (
        {
            "context": busca_vectorstore | format_docs,  # Busca documentos e formata
            "question": RunnablePassthrough()  # Passa a pergunta sem modificar
        }
        | prompt_inicial  # Aplica o template de prompt
        | chat_instance  # Envia para o modelo GPT
        | StrOutputParser()  # Extrai apenas o texto da resposta
)

# PASSO 2: Chain de tradu√ß√£o
# Cria uma chain espec√≠fica para tradu√ß√£o
translation_chain = translation_prompt | chat_instance | StrOutputParser()


# PASSO 4: Fun√ß√£o que executa o pipeline
def qa_chain_complete(query):
    """
    Fun√ß√£o que executa o pipeline completo:
    1. Busca documentos relevantes
    2. Gera resposta em portugu√™s
    3. Traduz a resposta para ingl√™s
    4. Retorna ambas as vers√µes

    CORRE√á√ÉO: Agora passamos apenas a string da query, n√£o um dicion√°rio
    """
    # Primeiro: gera a resposta usando apenas a string da pergunta
    resposta_original = rag_chain.invoke(query)

    # Segundo: traduz a resposta
    resposta_traduzida = translation_chain.invoke({
        "text": resposta_original
    })

    # Terceiro: busca os documentos fonte para refer√™ncia
    source_docs = retriever(query, qtd_retriever)

    # Retorna tudo em um dicion√°rio
    return {
        "result": resposta_original,
        "translated": resposta_traduzida,
        "source_documents": source_docs
    }


# ================================================
# 10. INTERFACE DE CHAT
# ================================================

st.markdown("---")
st.header("üí¨ Converse com o projeto BIA!")

# Inicializa o hist√≥rico de mensagens se n√£o existir
if "messages" not in st.session_state:
    st.session_state.messages = []

# Mostra o hist√≥rico de mensagens
for msg in st.session_state.messages:
    if msg["role"] == "user":
        # Mensagem do usu√°rio
        with st.chat_message("user", avatar="üë§"):
            st.markdown(msg["content"])
    else:
        # ========== MODIFICA√á√ÉO NA EXIBI√á√ÉO ==========
        # Agora mostramos a resposta em duas abas: portugu√™s e ingl√™s
        with st.chat_message("assistant", avatar="ü§ñ"):
            # Cria duas abas para as vers√µes da resposta
            tab1, tab2 = st.tabs(["üáßüá∑ Portugu√™s", "üá∫üá∏ English"])

            # Aba 1: Resposta original em portugu√™s
            with tab1:
                st.markdown(msg["content"])

            # Aba 2: Resposta traduzida em ingl√™s
            with tab2:
                # Verifica se existe tradu√ß√£o (para mensagens antigas)
                if "translated" in msg:
                    st.markdown(msg["translated"])
                else:
                    st.info("Tradu√ß√£o n√£o dispon√≠vel para mensagens anteriores")

            # Mostra as fontes se existirem
            if "sources" in msg and msg["sources"]:
                with st.expander("üìö Fontes"):
                    for i, source in enumerate(msg["sources"], 1):
                        st.markdown(f"**Fonte {i}:**")
                        st.markdown(source.page_content[:200] + "...")
                        st.markdown(f"*P√°gina: {source.metadata.get('page', 'N/A')}*")
                        st.markdown("---")
        # =============================================

# Campo de entrada para nova pergunta
user_input = st.chat_input(
    "Digite sua pergunta sobre o documento...",
    key="user_input_field"
)

# Processa nova pergunta
if user_input:
    # Adiciona pergunta do usu√°rio ao hist√≥rico
    st.session_state.messages.append({
        "role": "user",
        "content": user_input
    })

    # Mostra a pergunta do usu√°rio
    with st.chat_message("user", avatar="üë§"):
        st.markdown(user_input)

    # ========== PROCESSAMENTO MODIFICADO ==========
    # Agora processa tanto a resposta quanto a tradu√ß√£o
    with st.chat_message("assistant", avatar="ü§ñ"):
        with st.spinner("üîç Buscando informa√ß√µes e traduzindo..."):
            # CORRE√á√ÉO: Passa apenas a string user_input
            result = qa_chain_complete(user_input)

            # Extrai as respostas
            answer = result["result"]
            translated = result["translated"]
            sources = result.get("source_documents", [])

        # Mostra as respostas em abas
        tab1, tab2 = st.tabs(["üáßüá∑ Portugu√™s", "üá∫üá∏ English"])

        with tab1:
            st.markdown(answer)

        with tab2:
            st.markdown(translated)

        # Mostra as fontes
        if sources:
            with st.expander("üìö Fontes"):
                for i, source in enumerate(sources, 1):
                    st.markdown(f"**Fonte {i}:**")
                    st.markdown(source.page_content[:200] + "...")
                    st.markdown(f"*P√°gina: {source.metadata.get('page', 'N/A')}*")
                    st.markdown("---")

        # Adiciona resposta ao hist√≥rico com ambas as vers√µes
        st.session_state.messages.append({
            "role": "assistant",
            "content": answer,
            "translated": translated,  # NOVO: guarda a tradu√ß√£o
            "sources": sources
        })
    # =============================================


