import os  # Permite acessar arquivos, pastas e configura√ß√µes do computador
import streamlit as st  # Framework para criar aplica√ß√µes web interativas
from langchain_community.document_loaders import PyPDFLoader  # Carrega arquivos PDF
from langchain.text_splitter import RecursiveCharacterTextSplitter  # Divide textos grandes em peda√ßos menores
from langchain_openai import OpenAIEmbeddings  # Converte texto em n√∫meros (vetores)
from langchain_community.vectorstores import FAISS  # Banco de dados para armazenar e buscar vetores
from langchain_openai import ChatOpenAI  # Modelo de linguagem da OpenAI (GPT)
from langchain.chains import RetrievalQA  # Sistema de perguntas e respostas
from langchain.prompts import ChatPromptTemplate  # Template para formatar perguntas ao modelo
import tempfile  # Cria arquivos tempor√°rios
from dotenv import load_dotenv  # Carrega senhas do arquivo .env (arquivo secreto)
import time  # Controla tempo e delays

# ========== NOVOS IMPORTS PARA O PIPELINE COM TRADU√á√ÉO ==========
# Estes imports s√£o necess√°rios para criar o pipeline customizado
from langchain_core.runnables import RunnablePassthrough, RunnableParallel
from langchain_core.output_parsers import StrOutputParser

# =================================================================


# Tenta pegar a chave da API da OpenAI de duas formas diferentes
load_dotenv()
api_key = os.getenv('OPENAI_API_KEY')  # Do arquivo .env
# api_key = st.secrets["OPENAI_API_KEY"]  # Do Streamlit secrets (para deploy)

# Verifica se conseguiu pegar a chave da API
if not api_key:
    # Se n√£o conseguiu, para tudo e mostra erro
    raise ValueError("A vari√°vel de ambiente 'OPENAI_API_KEY' n√£o foi encontrada no seu arquivo .env.")

# Define a chave API como vari√°vel de ambiente
os.environ["OPENAI_API_KEY"] = api_key

#Define o modelo de GPT
model_name = 'gpt-3.5-turbo-0125'  # Qual modelo do ChatGPT usar

# 1. CONFIGURA√á√ÉO DA P√ÅGINA
# -------------------------
# Define t√≠tulo, √≠cone e layout da p√°gina
st.set_page_config(
    page_title="Assistente de IA: Biometria Inteligente ",  # T√≠tulo que aparece na aba do navegador
    page_icon="üêü",  # √çcone que aparece na aba
    layout="wide"  # Usa toda a largura da tela
)

# 2. T√çTULO E DESCRI√á√ÉO
# ---------------------
# Mostra o t√≠tulo principal e explica o que a aplica√ß√£o faz
st.title("üêü Biometria Inteligente para Aquicultura")
st.markdown("""
Esta aplica√ß√£o permite que voc√™ analise o projeto BIA usando Intelig√™ncia Artificial.
Fa√ßa perguntas sobre sua aplica√ß√£o, benef√≠cios e indicadores financeiros!
""")

# 3. CONFIGURA√á√ÉO DA BARRA LATERAL
# --------------------------------
# Cria uma barra lateral para configura√ß√µes
with st.sidebar:
    st.header("‚öôÔ∏è Configura√ß√µes")

    # Controle de temperatura (criatividade do modelo)
    temperature = st.slider(
        "Temperatura",
        min_value=0.0,
        max_value=1.0,
        value=0.1,
        step=0.1,
        help="0 = Respostas mais precisas, 1 = Respostas mais criativas"
    )

    # Upload do arquivo PDF
    uploaded_file = st.file_uploader(
        "üìÑ Fa√ßa upload do PDF",
        type=['pdf'],
        help="Selecione um arquivo PDF para an√°lise"
    )

# 4. PROCESSAMENTO DO PDF
# -----------------------
# Se um arquivo foi carregado, processa ele
if uploaded_file is not None:
    # Mostra informa√ß√µes sobre o arquivo
    st.success(f"‚úÖ Arquivo '{uploaded_file.name}' carregado com sucesso!")

    # Cria um arquivo tempor√°rio para salvar o PDF
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
        # Escreve o conte√∫do do upload no arquivo tempor√°rio
        tmp_file.write(uploaded_file.getvalue())
        tmp_file_path = tmp_file.name

    # Barra de progresso para mostrar o processamento
    with st.spinner("üìñ Lendo e processando o PDF..."):
        # Carrega o PDF
        loader = PyPDFLoader(tmp_file_path)
        documents = loader.load()

        # Mostra quantas p√°ginas foram carregadas
        st.info(f"üìÑ {len(documents)} p√°ginas carregadas do PDF")

    # 6. DIVIS√ÉO DO TEXTO EM CHUNKS
    # -----------------------------
    # Divide o texto em peda√ßos menores para melhor processamento
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,  # Tamanho m√°ximo de cada peda√ßo
        chunk_overlap=200,  # Sobreposi√ß√£o entre peda√ßos
        length_function=len,  # Fun√ß√£o para medir o tamanho
        separators=["&","\n\n", "\n", " ", ""]  # Onde dividir o texto
    )

    # Aplica a divis√£o
    chunks = text_splitter.split_documents(documents)
    st.info(f"‚úÇÔ∏è Documento dividido em {len(chunks)} peda√ßos")

    # 7. CRIA√á√ÉO DOS EMBEDDINGS E ARMAZENAMENTO
    # -----------------------------------------
    with st.spinner("üßÆ Criando embeddings e armazenando no banco vetorial..."):
        # Cria o modelo de embeddings
        embeddings = OpenAIEmbeddings()

        # Cria o banco de dados vetorial com os chunks
        vectorstore = FAISS.from_documents(chunks, embeddings)

        # Cria um buscador que retorna os 4 peda√ßos mais relevantes
        retriever = vectorstore.as_retriever(search_kwargs={"k": 4})

    st.success("‚úÖ Processamento conclu√≠do! Pronto para responder perguntas.")

    # 8. CONFIGURA√á√ÉO DO MODELO DE LINGUAGEM
    # --------------------------------------
    # Configura o ChatGPT com as configura√ß√µes escolhidas
    llm = ChatOpenAI(
        model_name=model_name,
        temperature=temperature,
        max_tokens=1000  # Limite de tokens na resposta
    )

    # Template para formatar as perguntas
    prompt = ChatPromptTemplate.from_template("""
    Use o contexto fornecido para responder √† pergunta. 
    Se voc√™ n√£o souber a resposta, diga que n√£o sabe, n√£o tente inventar.

    Contexto: {context}

    Pergunta: {question}

    Resposta detalhada:""")


    # ========== NOVA FUN√á√ÉO AUXILIAR ==========
    # Esta fun√ß√£o formata os documentos encontrados em um texto √∫nico
    def format_docs(docs):
        """
        Pega uma lista de documentos e junta todo o conte√∫do em um texto s√≥.
        Cada documento √© separado por duas quebras de linha.
        """
        return "\n\n".join(doc.page_content for doc in docs)


    # ==========================================

    # ========== NOVO PIPELINE COM TRADU√á√ÉO ==========
    # Substitu√≠mos o qa_chain original por um pipeline customizado
    # que faz tanto a busca/resposta quanto a tradu√ß√£o

    # PASSO 1: Chain que faz o RAG (busca + resposta)
    rag_chain = (
            {
                "context": retriever | format_docs,  # Busca documentos e formata
                "question": RunnablePassthrough()  # Passa a pergunta sem modificar
            }
            | prompt  # Aplica o template de prompt
            | llm  # Envia para o modelo GPT
            | StrOutputParser()  # Extrai apenas o texto da resposta
    )

    # PASSO 2: Template para tradu√ß√£o
    # Este prompt instrui o modelo a traduzir mantendo termos t√©cnicos
    translation_prompt = ChatPromptTemplate.from_template("""
    Voc√™ √© um tradutor especializado em textos t√©cnicos.
    Traduza o seguinte texto para ingl√™s, mantendo termos t√©cnicos e formata√ß√£o:

    Texto: {text}

    Tradu√ß√£o:""")

    # PASSO 3: Chain de tradu√ß√£o
    # Cria uma chain espec√≠fica para tradu√ß√£o
    translation_chain = translation_prompt | llm | StrOutputParser()


    # PASSO 4: Fun√ß√£o que executa todo o pipeline
    def qa_chain_complete(query):
        """
        Fun√ß√£o que executa o pipeline completo:
        1. Busca documentos relevantes
        2. Gera resposta em portugu√™s
        3. Traduz a resposta para ingl√™s
        4. Retorna ambas as vers√µes

        CORRE√á√ÉO: Agora passamos apenas a string da query, n√£o um dicion√°rio
        """
        # Primeiro: gera a resposta usando apenas a string da pergunta
        resposta_original = rag_chain.invoke(query)  # CORRE√á√ÉO: passa s√≥ a string

        # Segundo: traduz a resposta
        resposta_traduzida = translation_chain.invoke({
            "text": resposta_original
        })

        # Terceiro: busca os documentos fonte para refer√™ncia
        # CORRE√á√ÉO: usa a string query diretamente
        source_docs = retriever.get_relevant_documents(query)

        # Retorna tudo em um dicion√°rio
        return {
            "result": resposta_original,
            "translated": resposta_traduzida,
            "source_documents": source_docs
        }


    # ================================================

    # 10. INTERFACE DE CHAT
    # --------------------
    st.markdown("---")
    st.header("üí¨ Chat com o Documento")

    # Inicializa o hist√≥rico de mensagens se n√£o existir
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # Mostra o hist√≥rico de mensagens
    for msg in st.session_state.messages:
        if msg["role"] == "user":
            # Mensagem do usu√°rio
            with st.chat_message("user", avatar="üë§"):
                st.markdown(msg["content"])
        else:
            # ========== MODIFICA√á√ÉO NA EXIBI√á√ÉO ==========
            # Agora mostramos a resposta em duas abas: portugu√™s e ingl√™s
            with st.chat_message("assistant", avatar="ü§ñ"):
                # Cria duas abas para as vers√µes da resposta
                tab1, tab2 = st.tabs(["üáßüá∑ Portugu√™s", "üá∫üá∏ English"])

                # Aba 1: Resposta original em portugu√™s
                with tab1:
                    st.markdown(msg["content"])

                # Aba 2: Resposta traduzida em ingl√™s
                with tab2:
                    # Verifica se existe tradu√ß√£o (para mensagens antigas)
                    if "translated" in msg:
                        st.markdown(msg["translated"])
                    else:
                        st.info("Tradu√ß√£o n√£o dispon√≠vel para mensagens anteriores")

                # Mostra as fontes se existirem
                if "sources" in msg and msg["sources"]:
                    with st.expander("üìö Fontes"):
                        for i, source in enumerate(msg["sources"], 1):
                            st.markdown(f"**Fonte {i}:**")
                            st.markdown(source.page_content[:200] + "...")
                            st.markdown(f"*P√°gina: {source.metadata.get('page', 'N/A')}*")
                            st.markdown("---")
            # =============================================

    # Campo de entrada para nova pergunta
    user_input = st.chat_input(
        "Digite sua pergunta sobre o documento...",
        key="user_input_field"
    )

    # Processa nova pergunta
    if user_input:
        # Adiciona pergunta do usu√°rio ao hist√≥rico
        st.session_state.messages.append({
            "role": "user",
            "content": user_input
        })

        # Mostra a pergunta do usu√°rio
        with st.chat_message("user", avatar="üë§"):
            st.markdown(user_input)

        # ========== PROCESSAMENTO MODIFICADO ==========
        # Agora processa tanto a resposta quanto a tradu√ß√£o
        with st.chat_message("assistant", avatar="ü§ñ"):
            with st.spinner("üîç Buscando informa√ß√µes e traduzindo..."):
                # CORRE√á√ÉO: Passa apenas a string user_input
                result = qa_chain_complete(user_input)

                # Extrai as respostas
                answer = result["result"]
                translated = result["translated"]
                sources = result.get("source_documents", [])

            # Mostra as respostas em abas
            tab1, tab2 = st.tabs(["üáßüá∑ Portugu√™s", "üá∫üá∏ English"])

            with tab1:
                st.markdown(answer)

            with tab2:
                st.markdown(translated)

            # Mostra as fontes
            if sources:
                with st.expander("üìö Fontes"):
                    for i, source in enumerate(sources, 1):
                        st.markdown(f"**Fonte {i}:**")
                        st.markdown(source.page_content[:200] + "...")
                        st.markdown(f"*P√°gina: {source.metadata.get('page', 'N/A')}*")
                        st.markdown("---")

            # Adiciona resposta ao hist√≥rico com ambas as vers√µes
            st.session_state.messages.append({
                "role": "assistant",
                "content": answer,
                "translated": translated,  # NOVO: guarda a tradu√ß√£o
                "sources": sources
            })
        # =============================================

    # Remove arquivo tempor√°rio
    os.unlink(tmp_file_path)

else:
    # Se nenhum arquivo foi carregado
    st.info("üëà Por favor, fa√ßa upload de um arquivo PDF na barra lateral para come√ßar.")